{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV Data Challenge - BGS\n",
    "\n",
    "The purpose of this notebook is to simulate DESI Survey Validation (SV) observations of a single GAMA field, as a test of the BGS component of SV.\n",
    "\n",
    "A good set of reference files from the [end-to-end minitest](https://github.com/desihub/desitest/blob/master/mini/minitest.ipynb) notebook are located in */global/cscratch1/sd/sjbailey/desi/dev/end2end*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, time, subprocess, time\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, hstack, Column\n",
    "from astropy.io import fits\n",
    "\n",
    "import fitsio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting at {}\".format(time.asctime()))\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some environment variables and create output directories\n",
    "\n",
    "Define and create directories under `$DESI_ROOT/survey-validation` and set environment variables for this mapping:\n",
    "\n",
    "| Directory             | NB variable   | Environment Variable                              |\n",
    "|-----------------------|---------------|---------------------------------------------------|\n",
    "| survey/               | surveydir     | \\$DESISURVEY_OUTPUT                               |\n",
    "| targets/              | targetdir     |                                                   |\n",
    "| fiberassign/          | fibassigndir  |                                                   |\n",
    "| spectro/redux/mini/   | reduxdir      | \\$DESI_SPECTRO_REDUX/\\$SPECPROD                   |\n",
    "| spectro/sim/mini/     | simdatadir    | \\$DESI_SPECTRO_DATA = \\$DESI_SPECTRO_SIM/$PIXPROD |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir      = os.path.join(os.getenv('DESI_ROOT'), 'survey-validation')\n",
    "\n",
    "surveydir    = os.path.join(basedir, 'survey')\n",
    "targetdir    = os.path.join(basedir, 'targets')\n",
    "\n",
    "fibassigndir = os.path.join(basedir, 'fiberassign')\n",
    "\n",
    "os.environ['DESISURVEY_OUTPUT']  = surveydir\n",
    "os.environ['DESI_SPECTRO_REDUX'] = os.path.join(basedir, 'spectro', 'redux')\n",
    "os.environ['DESI_SPECTRO_SIM']   = os.path.join(basedir, 'spectro', 'sim')\n",
    "\n",
    "os.environ['PIXPROD']  = 'sv-bgs'\n",
    "os.environ['SPECPROD'] = 'sv-bgs'\n",
    "\n",
    "reduxdir   = os.path.join(os.environ['DESI_SPECTRO_REDUX'], os.environ['SPECPROD'])\n",
    "simdatadir = os.path.join(os.environ['DESI_SPECTRO_SIM'], os.environ['PIXPROD'])\n",
    "\n",
    "os.environ['DESI_SPECTRO_DATA'] = simdatadir\n",
    "\n",
    "for dd in (surveydir, targetdir, fibassigndir, reduxdir, simdatadir):\n",
    "    os.makedirs(dd, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplefile       = os.path.join(basedir, 'sv-bgs-sample.fits')\n",
    "tilesfile        = os.path.join(os.getenv('DESIMODEL'), 'data', 'footprint', 'sv-bgs-tiles.fits')\n",
    "surveyconfigfile = os.path.join(basedir, 'sv-bgs-survey-config.yaml')\n",
    "expfile          = os.path.join(surveydir, 'exposures.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_sample = True\n",
    "overwrite_tiles = True\n",
    "overwrite_surveysim = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and visualize the GAMA/BGS sample.\n",
    "\n",
    "Here we select a magnitude-limited (BGS-like) sample of GAMA targets in the G02 field with high-quality redshifts and Legacy Surveys photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_legacysurvey_gama(faintcut=20, overwrite=False):\n",
    "    \"\"\"\n",
    "    Read the GAMA/DR3 catalog and select the G02 field.\n",
    "    \"\"\"\n",
    "    truthdir = os.path.join(os.getenv('DESI_ROOT'), 'target', 'analysis', 'truth')\n",
    "    \n",
    "    def select_G02(cat, phot=None, return_index=False, faintcut=20):\n",
    "        \"\"\"Select the G02 field where it is complete.\n",
    "        See Table 1 in https://arxiv.org/pdf/1506.08222.pdf\n",
    "        \"\"\"\n",
    "        cut = ( (cat['RA'] > 30) * (cat['RA'] < 39) * \n",
    "                (cat['DEC'] >= -6) * (cat['DEC'] <= -4)\n",
    "              )\n",
    "        if phot:\n",
    "            cut = ( cut * (cat['NQ'] >= 3) * #(cat['Z'] < 1) * \n",
    "                    (phot['FLUX_R'] > 10**((22.5 - faintcut) / 2.5)) \n",
    "                  )\n",
    "            print('Selecting {} objects in the G02 field with NQ>=3 and r<{:.1f}.'.format(\n",
    "                np.sum(cut), faintcut))\n",
    "        else:\n",
    "            print('Selecting {} objects in the G02 field.'.format(np.sum(cut)))\n",
    "        \n",
    "        outcat = cat[cut]\n",
    "        if return_index:\n",
    "            return cut\n",
    "        else:\n",
    "            return outcat\n",
    "    \n",
    "    def read_all_gama():\n",
    "        \"\"\"Read all the objects in GAMA/DR3.\"\"\"\n",
    "        gamafile = os.path.join(truthdir, 'parent', 'GAMA-DR3-SpecObj.fits')\n",
    "        allgama = Table.read(gamafile)\n",
    "        print('Read {} objects from {}'.format(len(allgama), gamafile))\n",
    "        gama = select_G02(allgama)\n",
    "        return allgama, gama\n",
    "\n",
    "    def read_legacysurvey(faintcut=20):\n",
    "        \"\"\"Read the GAMA/DR3 catalog matched to LegacySurvey/DR5.\"\"\"\n",
    "        gamafile = os.path.join(truthdir, 'dr5.0', 'trimmed', 'GAMA-DR3-SpecObj-trim.fits')\n",
    "        alllsgama = Table.read(gamafile)\n",
    "        print('Read {} objects from {}'.format(len(alllsgama), gamafile))\n",
    "        \n",
    "        lsfile = os.path.join(truthdir, 'dr5.0', 'trimmed', 'decals-dr5.0-GAMA-DR3-SpecObj-trim.fits')\n",
    "        allls = Table.read(lsfile)\n",
    "        print('Read {} objects from {}'.format(len(allls), lsfile))\n",
    "        \n",
    "        # Trim to the G02 field and join the two catalogs.\n",
    "        indx = select_G02(alllsgama, phot=allls, return_index=True, faintcut=faintcut)\n",
    "        lsgama = hstack( (allls[indx], alllsgama[indx]) )\n",
    "        lsgama.rename_column('RA_1', 'RA')\n",
    "        lsgama.rename_column('DEC_1', 'DEC')\n",
    "        lsgama.rename_column('RA_2', 'RA_GAMA')\n",
    "        lsgama.rename_column('DEC_2', 'DEC_GAMA')\n",
    "        \n",
    "        return lsgama\n",
    "        \n",
    "    def get_rmag(cat):\n",
    "        return 22.5 - 2.5 * np.log10(cat['FLUX_R'] / cat['MW_TRANSMISSION_R'])\n",
    "        \n",
    "    allgama, gama = read_all_gama()\n",
    "    lsgama = read_legacysurvey(faintcut=faintcut)\n",
    "    \n",
    "    if overwrite:\n",
    "        print('Writing {} GAMA/BGS targets to {}'.format(len(lsgama), samplefile))\n",
    "        lsgama.write(samplefile, overwrite=overwrite)\n",
    "    \n",
    "    allgama_G02 = allgama[(allgama['RA'] > 30) * (allgama['RA'] < 39)]\n",
    "    \n",
    "    # Visualize the position of the G02 field.\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax1.scatter(allgama['RA'], allgama['DEC'], s=1)\n",
    "    ax1.set_title('All GAMA Fields (N={})'.format(len(allgama)))\n",
    "    \n",
    "    ax2.scatter(allgama_G02['RA'], allgama_G02['DEC'], s=1, label='All Objects (N={})'.format(len(allgama_G02)))\n",
    "    ax2.scatter(gama['RA'], gama['DEC'], s=1, label='Complete Region (N={})'.format(len(gama)))\n",
    "    ax2.set_xlim(29, 40)\n",
    "    ax2.set_ylim(-11, -2)\n",
    "    ax2.set_title('GAMA/G02 Field')\n",
    "    ax2.legend(loc='upper left', ncol=1, markerscale=5)\n",
    "    \n",
    "    ax3.scatter(lsgama['RA'], lsgama['DEC'], s=1, alpha=0.5, \n",
    "                label='NQ>=3, r<{:.1f} (N={})'.format(faintcut, len(lsgama)))\n",
    "    ax3.set_xlim(29, 40)\n",
    "    ax3.set_ylim(-7, -3)\n",
    "    ax3.set_title('GAMA/G02 Field')\n",
    "    ax3.legend(loc='upper left', markerscale=5)\n",
    "\n",
    "    for ax in (ax1, ax2, ax3):\n",
    "        ax.set_xlabel('RA')\n",
    "        ax.set_ylabel('Dec')\n",
    "        \n",
    "    fig.subplots_adjust(wspace=0.25)\n",
    "    \n",
    "    if overwrite:\n",
    "        fig.savefig(os.path.join(basedir, 'qa-sv-bgs-all.png'))\n",
    "    \n",
    "    # Visualize the magnitude and redshift distributions.\n",
    "    rmag = get_rmag(lsgama)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax1.hist(lsgama['Z'], bins=50)\n",
    "    ax1.set_xlabel('GAMA Redshift')\n",
    "    ax1.set_ylabel('Number of Objects')\n",
    "    \n",
    "    ax2.hist(rmag, bins=60)\n",
    "    ax2.set_xlabel(r'$r_{cor}$ (DECaLS, AB mag)')\n",
    "    ax2.set_ylabel('Number of Objects')\n",
    "    ax2.axvline(x=faintcut, color='k', ls='--')\n",
    "    \n",
    "    ax3.scatter(lsgama['Z'], rmag, s=1)\n",
    "    ax3.set_xlabel('GAMA Redshift')\n",
    "    ax3.set_ylabel(r'$r_{cor}$ (DECaLS, AB mag)')\n",
    "    ax3.set_ylim(12, 20)\n",
    "    ax3.axhline(y=faintcut, color='k', ls='--')\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.25)\n",
    "    \n",
    "    if overwrite:\n",
    "        fig.savefig(os.path.join(basedir, 'qa-sv-bgs.png'))\n",
    "    \n",
    "    return lsgama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gama = read_legacysurvey_gama(faintcut=19.7, overwrite=overwrite_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gama[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile the GAMA/G02 BGS field\n",
    "\n",
    "Here we produce an overly simplistic three-tile (three-pointing) solution. This code and the corresponding output files should go into *desimodel.footprint*.\n",
    "\n",
    "From Eddie:\n",
    "\n",
    "The tilesra2s, tilesdec2s scheme below is also pretty good for filling in the holes in the DESI focal plane with 3 dithers, so any hexagonal-close-packed scheme with ~1.6 degree spacing and that dither pattern should be pretty good.\n",
    "\n",
    "I can recommend\n",
    "```\n",
    "tilesra2b, tilesdec2b = tiling.simpleradecoffscheme(desitiles['ra'][m0], desitiles['dec'][m0], dx=1.4, ang=330)\n",
    "tilesra2s, tilesdec2s = tiling.simpleradecoffscheme(desitiles['ra'][m0], desitiles['dec'][m0], dx=0.6, ang=42)\n",
    "```\n",
    "as providing a simple set of dithers that fill in almost all of the DESI focal plane holes at 3 dithers, given the DESI PASS 1 tiles as a base, though any ~hexagonal scheme with equal density of points to the DESI tiles PASS 1 polyhedron will work equally well.  These are just uniform offsets in ra and dec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleradecoffscheme(ras, decs, dx=0.6, ang=42):\n",
    "    # take a single covering, define 4 sets of offsets, \n",
    "    # start with something minimal: need to cover:\n",
    "    # central bulls-eyes: 0.2 deg\n",
    "    # GFA gaps: up to 0.4 deg\n",
    "    \n",
    "    # okay: \n",
    "    from numpy import sin, cos\n",
    "    ang = numpy.radians(ang)\n",
    "    dang = numpy.pi/2\n",
    "    dithers = [[0, 0], \n",
    "               [dx*sin(ang+0*dang), dx*cos(ang+0*dang)], \n",
    "               [dx*sin(ang+1*dang), dx*cos(ang+1*dang)],\n",
    "               [dx*sin(ang+2*dang), dx*cos(ang+2*dang)]]\n",
    "    dithers = numpy.cumsum(numpy.array(dithers), axis=0)\n",
    "    dithers = list(dithers) + [[numpy.mean([d[0] for d in dithers]),\n",
    "                                numpy.mean([d[1] for d in dithers])]]\n",
    "    fac = 1./numpy.cos(numpy.radians(decs))\n",
    "    fac = numpy.clip(fac, 1, 360*5)  # confusion near celestial pole.\n",
    "    newras = numpy.concatenate([ras+d[0]*fac for d in dithers])\n",
    "    newdecs = numpy.concatenate([decs+d[1] for d in dithers])\n",
    "    newdecs = numpy.clip(newdecs, -numpy.inf, 90.)\n",
    "    newras = newras % 360\n",
    "    newras = numpy.concatenate([newras, newras])\n",
    "    newdecs = numpy.concatenate([newdecs, newdecs])\n",
    "    return newras, newdecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tile(ra, dec, r=1.606, color='k'):\n",
    "    '''Approximate plot of tile location'''\n",
    "    ang = np.linspace(0, 2*np.pi, 100)\n",
    "    x = ra + r*np.cos(ang)/np.cos(np.radians(dec))\n",
    "    y = dec + r*np.sin(ang)\n",
    "    plt.plot(x,y, '-', color=color, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_tiles(cat):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(cat['RA'], cat['DEC'], s=1, alpha=0.5)\n",
    "    for tt in tiles:\n",
    "        plot_tile(tt['RA'], tt['DEC'])\n",
    "    ax.set_xlabel('RA')\n",
    "    ax.set_ylabel('Dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gama_bgs_footprint(overwrite=False):\n",
    "    \"\"\"The tiling solution needs to be done properly and the code \n",
    "    needs to be moved elsewhere.\n",
    "    \n",
    "    \"\"\"\n",
    "    ntile = 3\n",
    "    tiles = Table()\n",
    "    tiles.add_column(Column(name='TILEID', dtype=np.int32, length=ntile))\n",
    "    tiles.add_column(Column(name='RA', dtype='f8', length=ntile))\n",
    "    tiles.add_column(Column(name='DEC', dtype='f8', length=ntile))\n",
    "    tiles.add_column(Column(name='PASS', dtype=np.int16, length=ntile))\n",
    "    tiles.add_column(Column(name='IN_DESI', dtype=np.int16, length=ntile))\n",
    "    tiles.add_column(Column(name='EBV_MED', dtype='f4', length=ntile))\n",
    "    tiles.add_column(Column(name='AIRMASS', dtype='f4', length=ntile))\n",
    "    tiles.add_column(Column(name='STAR_DENSITY', dtype='f4', length=ntile))\n",
    "    tiles.add_column(Column(name='EXPOSEFAC', dtype='f4', length=ntile))\n",
    "    tiles.add_column(Column(name='PROGRAM', dtype='S4', length=ntile))\n",
    "    tiles.add_column(Column(name='OBSCONDITIONS', dtype=np.int32, length=ntile))\n",
    "    \n",
    "    tiles['RA'] = [31.5, 34.3, 37.1]\n",
    "    tiles['DEC'] = [-4.8, -5.2, -5.0]\n",
    "    tiles['TILEID'] = [100000, 100001, 100002]\n",
    "    tiles['PASS'] = 1\n",
    "    tiles['AIRMASS'] = 1.0\n",
    "    tiles['IN_DESI'] = 1 # assume all \"in DESI\"\n",
    "    tiles['EXPOSEFAC'] = 1.0\n",
    "    tiles['PROGRAM'] = 'DARK'\n",
    "    tiles['OBSCONDITIONS'] = 1\n",
    "    \n",
    "    if overwrite:\n",
    "        print('Writing {}'.format(tilesfile))\n",
    "        tiles.write(tilesfile, overwrite=overwrite)\n",
    "    \n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = gama_bgs_footprint(overwrite=overwrite_tiles)\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tiles(gama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run survey simulations\n",
    "\n",
    "Simulate SV observing.\n",
    "\n",
    "One outstanding question is whether we need a new set of rules when calling *surveyplan* other than those in [desisurvey/data](https://github.com/desihub/desisurvey/tree/master/py/desisurvey/data).\n",
    "\n",
    "Note that we convert *progress.fits*, which is one row per tile, to *exposures.fits*, which has one row per exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_simulations(overwrite=False):\n",
    "    \"\"\"Run survey simulations.\n",
    "    \n",
    "    \"\"\"\n",
    "    if overwrite or not os.path.isfile(expfile):\n",
    "        from desisurvey.progress import Progress\n",
    "        from desisurvey.config import Configuration\n",
    "        from surveysim.util import add_calibration_exposures\n",
    "    \n",
    "        Configuration.reset()\n",
    "        config = Configuration(surveyconfigfile)\n",
    "\n",
    "        survey_logname = os.path.join(surveydir, 'survey.log')\n",
    "        print('Running survey simulations; logging to {}'.format(survey_logname))\n",
    "        \n",
    "        with open(survey_logname, 'w') as logfile:\n",
    "            cmd = \"surveyinit --config-file {} --output-path {}\".format(surveyconfigfile, surveydir)\n",
    "            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "            assert err == 0\n",
    "\n",
    "            # Use 0d fiber assignment delay to move on with mini sim quickly\n",
    "            # Do we need new rules?!?\n",
    "            cmd = \"surveyplan --config-file {} --output-path {} --create --fa-delay 0d\".format(\n",
    "                surveyconfigfile, surveydir)\n",
    "            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "            assert err == 0\n",
    "\n",
    "            cmd = \"surveysim --config-file {} --output-path {} --seed {}\".format(\n",
    "                surveyconfigfile, surveydir, seed)\n",
    "            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "            assert err == 0\n",
    "\n",
    "            # Do we need new rules?!?\n",
    "            plan_cmd = 'surveyplan --config-file {} --output-path {} --fa-delay 0d'.format(\n",
    "                surveyconfigfile, surveydir)\n",
    "            sim_cmd = 'surveysim --resume --config-file {} --output-path {} --seed {}'.format(\n",
    "                surveyconfigfile, surveydir, seed)\n",
    "            while True:\n",
    "                lastdate = open(os.path.join(surveydir, 'last_date.txt')).readline().strip()\n",
    "                progress = Table.read(os.path.join(surveydir, 'progress.fits'), 1)\n",
    "                ndone = np.count_nonzero(progress['status'] == 2)\n",
    "                print('Starting {} with {}/{} tiles completed {}'.format(lastdate, ndone, len(progress), time.asctime()))\n",
    "                if subprocess.call(plan_cmd.split(), stdout=logfile, stderr=logfile) != 0:\n",
    "                    break\n",
    "                if subprocess.call(sim_cmd.split(), stdout=logfile, stderr=logfile) != 0:\n",
    "                    break\n",
    "\n",
    "        # Make sure observing truly finished.\n",
    "        progressfile = os.path.join(surveydir, 'progress.fits')\n",
    "        if not os.path.exists(progressfile):\n",
    "            print(\"ERROR: Missing {}\".format(progressfile))\n",
    "            print(\"Check {} for what might have gone wrong\".format(survey_logname))\n",
    "    \n",
    "        print('Files in {}:\\n'.format(surveydir))\n",
    "        !ls $surveydir\n",
    "        \n",
    "        # convert progress.fits -> exposures.fits\n",
    "        p = Progress(restore='progress.fits')\n",
    "        explist = p.get_exposures()\n",
    "        explist = add_calibration_exposures(explist)\n",
    "\n",
    "        # Sanity check that all tiles in the subset were observed in the exposures list.\n",
    "        if not np.all(np.in1d(tiles['TILEID'], explist['TILEID'])):\n",
    "            print(\"ERROR: some tiles weren't observed;\\ncheck {} for failures\".format(survey_logname) )\n",
    "            print(\"Missing TILEIDs:\", set(tiles['TILEID']) - set(explist['TILEID']))\n",
    "        else:\n",
    "            print('All tiles in the subset were observed at least once.')\n",
    "            explist.write(expfile, overwrite=True)\n",
    "            print('Writing {}'.format(expfile))                \n",
    "\n",
    "        # Optionally make a movie\n",
    "        if False:\n",
    "            cmd = \"surveymovie --config-file {} --output-path {}\".format(\n",
    "                surveyconfigfile, surveydir)\n",
    "            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "            assert err == 0\n",
    "    else:\n",
    "        print('Simulated observing has been done.')\n",
    "        explist = Table.read(expfile)\n",
    "        print('Read {} exposures from {}'.format(len(explist), expfile))\n",
    "\n",
    "    return explist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time explist = survey_simulations(overwrite=overwrite_surveysim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize which healpixels cover the observed tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiles2pixels(nside=64):\n",
    "    import desimodel.footprint\n",
    "    pixels = desimodel.footprint.tiles2pix(nside, tiles)\n",
    "    nexp = np.count_nonzero(np.in1d(explist['TILEID'], tiles['TILEID']))\n",
    "    print('{} tiles covered by {} exposures and {} nside={} healpixels'.format(\n",
    "        len(tiles), nexp, len(pixels), nside))\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_healpix(nside, pixels, ax=None):\n",
    "    '''Plot healpix boundaries; doesn't work at RA wraparound'''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    for p in pixels:\n",
    "        xyz = hp.boundaries(nside, p, nest=True)\n",
    "        theta, phi = hp.vec2ang(xyz.T)\n",
    "        theta = np.concatenate([theta, theta[0:1]])\n",
    "        phi = np.concatenate([phi, phi[0:1]])\n",
    "        ra, dec = np.degrees(phi), 90-np.degrees(theta)\n",
    "        ax.plot(ra, dec, '-', color='0.6') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_observed_tiles():\n",
    "    isbright = explist['PROGRAM'] == 'BRIGHT'\n",
    "    isgray = explist['PROGRAM'] == 'GRAY'\n",
    "    isdark = explist['PROGRAM'] == 'DARK'\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(tiles['RA'], tiles['DEC'], 'k.', alpha=0.2, label='_none_')\n",
    "    ax.plot(explist['RA'][isdark], explist['DEC'][isdark], 'o', color='k', ms=10, mew=2, label='dark')\n",
    "    if np.sum(isgray) > 0:\n",
    "        ax.plot(explist['RA'][isgray], explist['DEC'][isgray], 's', \n",
    "                color='0.6', ms=10, label='gray')\n",
    "    if np.sum(isbright) > 0:\n",
    "        ax.plot(explist['RA'][isbright], explist['DEC'][isbright], 'd', \n",
    "                color='m', ms=10, mew=2, label='bright')\n",
    "    ax.legend(loc='upper right')\n",
    "    #ax.set_xlim(148, 157)\n",
    "    #ax.set_ylim(28, 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_tiles2pixels(nside=64):\n",
    "    \n",
    "    pixels = tiles2pixels(nside=nside)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "    ax1.plot(tiles['RA'], tiles['DEC'], 'rx')\n",
    "    ax1.plot(explist['RA'], explist['DEC'], 'b.', alpha=0.5)\n",
    "    #xlim(0,360); ylim(-20, 80)\n",
    "\n",
    "    plot_healpix(nside, pixels, ax=ax2)\n",
    "    color = dict(DARK='k', GRAY='b', BRIGHT='m')\n",
    "    for program in ['DARK', 'GRAY', 'BRIGHT']:\n",
    "        ii = tiles['PROGRAM'] == program\n",
    "        ax2.plot(tiles['RA'][ii], tiles['DEC'][ii], '.', color=color[program], alpha=0.5)\n",
    "        jj = tiles['PROGRAM'] == program\n",
    "        for t in tiles[jj]:\n",
    "            plot_tile(t['RA'], t['DEC'], color=color[program])\n",
    "    #xlim(143, 161); ylim(28, 37)\n",
    "    \n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = qa_tiles2pixels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run select_mock_targets\n",
    "\n",
    "This step combines mock catalogs with spectral templates per covered healpixel.\n",
    "This notebook prints the commands to run in a separate cori login terminal to\n",
    "get a 15 node interactive job, and then run the `mpi_select_mock_targets` command.\n",
    "It should take ~15 minutes on 15 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_select_mock_targets_done(targetdir, pixels, verbose=False):\n",
    "    done = True\n",
    "    for filetype in ['target', 'truth', 'sky', 'standards-dark', 'standards-bright']:\n",
    "        filenames = glob.glob(os.path.join(targetdir, '*', '*', '{}*.fits'.format(filetype)))\n",
    "        if verbose:\n",
    "            print('{}/{} {} files'.format(len(filenames), len(pixels), filetype))\n",
    "        if len(filenames) != len(pixels):\n",
    "            done = False\n",
    "    return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_select_mock_targets_done(targetdir, pixels):\n",
    "    print(\"mpi_select_mock_targets already successfully run; skipping\")\n",
    "else:\n",
    "    logfilename = os.path.join(targetdir, 'select_mock_targets.log')\n",
    "    tilefile = os.path.join(targetdir, 'test-tiles.fits')\n",
    "    print('Running mpi_select_mock_targets batch job; this should take ~15 minutes.')\n",
    "    print(\"Starting at {}\".format(time.asctime()))\n",
    "    print('Logging to {}'.format(logfilename))\n",
    "\n",
    "    # configfile = os.path.join(os.getenv('DESITARGET'), 'doc', 'mock_example', 'input.yaml')\n",
    "    configfile = './select-mock-targets.yaml'\n",
    "    assert os.path.exists(configfile)\n",
    "    \n",
    "    cmd = \"srun -A desi -N 15 -n 30 -c 16 -C haswell -t 00:30:00 --qos interactive\"\n",
    "    cmd += \" mpi_select_mock_targets --output_dir {targetdir} --config {configfile}\"\n",
    "    cmd += \" --seed {} --nproc 16 --nside 64 --tiles {tilefile}\".format(seed)\n",
    "    cmd = cmd.format(targetdir=targetdir, tilefile=tilefile, configfile=configfile)\n",
    "    \n",
    "    with open(logfilename, 'a') as logfile:\n",
    "        err = subprocess.call(cmd.split(), stderr=logfile, stdout=logfile)\n",
    "        if err != 0:\n",
    "            print('mpi_select_mock_targets failed err={}; see {}'.format(err, logfilename))\n",
    "        else:\n",
    "            print('done at {}'.format(time.asctime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that we got the right number of target output files ##\n",
    "The number of files of each type should match the number of healpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_select_mock_targets_done(targetdir, pixels, verbose=True):\n",
    "    print('Success')\n",
    "else:\n",
    "    print('ERROR: missing files')\n",
    "    print('Check {}'.format(logfilename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge target, sky, and stdstar catalogs\n",
    "\n",
    "mpi_select_mock_targets writes targets per healpixel.  This step combines them into full catalogs (but the truth spectra themselves are still kept in individual healpix-organized files, otherwise they would be too big).\n",
    "It also generates the \"Merged Target List\" (mtl) that assigns priorities to targets for fiberassignment.\n",
    "This step is fast so is spawned directly from the notebook without requiring a batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmd = \"srun -N 4 -n 8 -c 16 -p debug -C haswell -t 00:05:00\"\n",
    "cmd = \"join_mock_targets --mockdir {} --force\".format(targetdir)\n",
    "print(cmd)\n",
    "err = subprocess.call(cmd.split())\n",
    "if err != 0:\n",
    "    print('join_mock_targets failed err={}'.format(err))\n",
    "else:\n",
    "    print('success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some sanity checks on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = fitsio.read(os.path.join(targetdir, 'targets.fits'))\n",
    "truth   = fitsio.read(os.path.join(targetdir, 'truth.fits'))\n",
    "mtl     = fitsio.read(os.path.join(targetdir, 'mtl.fits'))\n",
    "std     = fitsio.read(os.path.join(targetdir, 'standards-dark.fits'))\n",
    "sky     = fitsio.read(os.path.join(targetdir, 'sky.fits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(truth) == len(targets)                             #- same number of targets and truth\n",
    "assert np.all(targets['TARGETID'] == truth['TARGETID'])       #- targets and truth are row matched\n",
    "assert len(targets) == len(np.unique(targets['TARGETID']))    #- no repeated TARGETIDs\n",
    "assert len(sky) == len(np.unique(sky['TARGETID']))            #- no repeated sky TARGETIDs\n",
    "assert len(std) == len(np.unique(std['TARGETID']))            #- no repeated std TARGETIDs\n",
    "\n",
    "assert np.all(np.in1d(targets['TARGETID'], mtl['TARGETID']))  #- all targets are in MTL\n",
    "\n",
    "#- no sky targets should be in science targets, though it is possible for standards to also be MWS targets\n",
    "assert not np.any(np.in1d(sky['TARGETID'], targets['TARGETID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(6,4))\n",
    "plot(mtl['RA'], mtl['DEC'], 'b,', alpha=0.1)\n",
    "plot(std['RA'], std['DEC'], 'm.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run target selection QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetfile = os.path.join(targetdir, 'targets.fits')\n",
    "targetQAdir = os.path.join(targetdir, 'qa')\n",
    "targetQAlog = os.path.join(targetQAdir, 'target-qa.log')\n",
    "os.makedirs(targetQAdir, exist_ok=True)\n",
    "\n",
    "cmd = 'run_target_qa {} {} --mocks --nside 32'.format(targetfile, targetQAdir)\n",
    "with open(targetQAlog, 'w') as logfile:\n",
    "    err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "\n",
    "if err != 0:\n",
    "    print('ERROR running {}'.format(cmd))\n",
    "    msg = 'see {}'.format(targetQAlog)\n",
    "    raise RuntimeError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "Image(targetQAdir+'/skymap-ALL.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fiberassign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fiberassign_done(fibassigndir, tiles, verbose=False):\n",
    "    done = True\n",
    "    for tileid in tiles['TILEID']:\n",
    "        tilefile = os.path.join(fibassigndir, 'tile_{:05d}.fits'.format(tileid))\n",
    "        if not os.path.exists(tilefile):\n",
    "            done = False\n",
    "            if verbose:\n",
    "                print('Missing {}'.format(tilefile))\n",
    "\n",
    "    return done\n",
    "\n",
    "# is_fiberassign_done(fibassigndir, tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_fiberassign_done(fibassigndir, tiles):\n",
    "    print('fiberassign already done; skipping')\n",
    "else:\n",
    "    print('Generating lists of dark and bright tiles')\n",
    "    #- Write list of tiles to consider\n",
    "    dx = open(os.path.join(fibassigndir, 'dark-tiles.txt'), 'w')\n",
    "    bx = open(os.path.join(fibassigndir, 'bright-tiles.txt'), 'w')\n",
    "    for tileid, program  in zip(tiles['TILEID'], tiles['PROGRAM']):\n",
    "        if program == 'BRIGHT':\n",
    "            bx.write(str(tileid)+'\\n')\n",
    "        else:\n",
    "            dx.write(str(tileid)+'\\n')\n",
    "\n",
    "    dx.close()\n",
    "    bx.close()\n",
    "\n",
    "    #- Remove any leftover tile files\n",
    "    for tilefile in glob.glob(fibassigndir+'/tile_*.fits'):\n",
    "        os.remove(tilefile)\n",
    "\n",
    "    cmd = \"fiberassign \"\n",
    "    cmd += \" --mtl {}/mtl.fits\".format(targetdir)\n",
    "    cmd += \" --stdstar {}/{{stdfile}}\".format(targetdir)\n",
    "    cmd += \" --sky {}/sky.fits\".format(targetdir)\n",
    "    cmd += \" --surveytiles {}/{{tilefile}}\".format(fibassigndir)\n",
    "    cmd += \" --footprint {}/data/footprint/desi-tiles.fits\".format(os.getenv('DESIMODEL'))\n",
    "    cmd += \" --positioners {}/data/focalplane/fiberpos.txt\".format(os.getenv('DESIMODEL'))\n",
    "    cmd += \" --fibstatusfile {}/fiberstatus.ecsv\".format(minitestdir)\n",
    "    cmd += \" --outdir {}\".format(fibassigndir)\n",
    "\n",
    "    #- Run fiberassign\n",
    "    logfilename = os.path.join(fibassigndir, 'fiberassign.log')\n",
    "    print('logging to {}'.format(logfilename))\n",
    "    with open(logfilename, 'a') as logfile:\n",
    "        for program in ['dark', 'bright']:\n",
    "            stdfile = 'standards-{}.fits'.format(program)\n",
    "            tilefile = '{}-tiles.txt'.format(program)\n",
    "            cmdx = cmd.format(stdfile=stdfile, tilefile=tilefile)\n",
    "            print('RUNNING', cmdx)\n",
    "            err = subprocess.call(cmdx.split(), stdout=logfile, stderr=logfile)\n",
    "            if err != 0:\n",
    "                print('fiberassign failed err={}; see {}'.format(err, logfilename))\n",
    "        \n",
    "    #- Run fiberassign\n",
    "#     logfilename = os.path.join(fibassigndir, 'fiberassign.log')\n",
    "#     print('Running fiberassign; logging to {}'.format(logfilename))\n",
    "#     with open(logfilename, 'a') as logfile:\n",
    "#         cmd = \"fiberassign {}/fiberassign-config-dark.txt\".format(fibassigndir)\n",
    "#         err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "#         if err != 0:\n",
    "#             print('fiberassign failed err={}; see {}'.format(err, logfilename))\n",
    "\n",
    "#         cmd = \"fiberassign {}/fiberassign-config-bright.txt\".format(fibassigndir)\n",
    "#         err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "#         if err != 0:\n",
    "#             print('fiberassign failed err={}; see {}'.format(err, logfilename))\n",
    "\n",
    "    if is_fiberassign_done(fibassigndir, tiles, verbose=True):\n",
    "        print('SUCCESS')\n",
    "    else:\n",
    "        print('ERROR: missing fiberassign output files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run fiberassign QA ####\n",
    "This will find non-fatal errors with unassigned fibers and too few standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qa-fiberassign $fibassigndir/tile*.fits --targets $targetdir/targets.fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run newexp to combine surveysim, mocks, and fiberassign into simspec files\n",
    "\n",
    "Setup the problem and define the commands to run, but then run them\n",
    "in a separate window with desisim/newexp configured.\n",
    "\n",
    "This step associates exposure IDs (EXPID) to observations and inserts 3 arcs and 3 flats\n",
    "at the beginning of each night.\n",
    "\n",
    "specsim is memory hungry so we are limited to one process per node, leaving\n",
    "the other cores idle.\n",
    "\n",
    "4.4 minutes for 12 arc, 12 flat, 18 science exposures on 15 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_newexp_done(explist, simdatadir, verbose=False):\n",
    "    numnights = len(set(explist['NIGHT']))\n",
    "    nexp = len(explist)  #- 3 arc/night + 3 flat/night + science\n",
    "    simspecfiles = glob.glob(simdatadir+'/*/simspec*.fits')\n",
    "    fibermapfiles = glob.glob(simdatadir+'/*/fibermap*.fits')\n",
    "    if verbose:\n",
    "        print('{}/{} simspec files'.format(len(simspecfiles), nexp))\n",
    "        print('{}/{} fibermap files'.format(len(fibermapfiles), nexp))\n",
    "\n",
    "    if len(simspecfiles) != nexp:\n",
    "        return False\n",
    "    elif len(fibermapfiles) != nexp:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# is_newexp_done(explist, simdatadir, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_newexp_done(explist, simdatadir):\n",
    "    print('newexp already done; skipping')\n",
    "else:\n",
    "    logfilename = os.path.join(simdatadir, 'newexp.log')\n",
    "    tilefile = os.path.join(targetdir, 'test-tiles.fits')\n",
    "\n",
    "    print('Running wrap-newexp batch job; should take ~5 min')\n",
    "    print('Starting at {}'.format(time.asctime()))\n",
    "    print('Logging to {}'.format(logfilename))\n",
    "    nodes = 15\n",
    "\n",
    "    cmd = \"srun -A desi -N {nodes} -n {nodes} -c 32\".format(nodes=nodes)\n",
    "    cmd += \" -C haswell -t 00:15:00 --qos interactive\"\n",
    "    cmd += \" wrap-newexp --mpi --fiberassign {}\".format(fibassigndir)\n",
    "    cmd += \" --mockdir {}\".format(targetdir)\n",
    "    cmd += \" --obslist {}/exposures.fits\".format(surveydir)\n",
    "    cmd += \" --tilefile {}\".format(tilefile)\n",
    "    # print(cmd)\n",
    "    \n",
    "    with open(logfilename, 'w') as logfile:\n",
    "        err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "        if err != 0:\n",
    "            print('ERROR {} running wrap-newexp; see {}'.format(err, logfilename))\n",
    "        else:\n",
    "            print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that we got the expected newexp output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_newexp_done(explist, simdatadir, verbose=True):\n",
    "    print('Success')\n",
    "else:\n",
    "    print('ERROR: Missing files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fastframe to generate noisy uncalibrated spectra\n",
    "\n",
    "fastframe is a stripped down version of quickgen, and it uses specsim under the hood.\n",
    "specsim is memory hungry so we are limited to one process per node, leaving\n",
    "the other cores idle.\n",
    "\n",
    "6.3 minutes for 12 arc, 12 flat, 18 science exposures on 15 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fastframe_done(explist, reduxdir, verbose=False):\n",
    "    nflat      = np.count_nonzero(explist['FLAVOR'] == 'flat')\n",
    "    nscience   = np.count_nonzero(explist['FLAVOR'] == 'science')\n",
    "    nframe     = 30*(nflat + nscience)\n",
    "    framefiles = glob.glob(reduxdir+'/exposures/*/*/frame*.fits')\n",
    "    \n",
    "    if verbose:\n",
    "        print('{}/{} frame files'.format(len(framefiles), nframe))\n",
    "    \n",
    "    if len(framefiles) != nframe:\n",
    "        return False\n",
    "    \n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# is_fastframe_done(explist, reduxdir, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_fastframe_done(explist, reduxdir, verbose=True):\n",
    "    print('fastframe already done; skipping')\n",
    "\n",
    "else:\n",
    "    logfilename = os.path.join(reduxdir, 'exposures', 'fastframe.log')\n",
    "    os.makedirs(os.path.dirname(logfilename), exist_ok=True)\n",
    "    print('Running fastframe batch job; should take ~7 min')\n",
    "    print('Starting at {}'.format(time.asctime()))\n",
    "    print('Logging to {}'.format(logfilename))\n",
    "    nodes = 15\n",
    "\n",
    "    cmd = \"srun -A desi -N {nodes} -n {nodes} -c 32 -C haswell -t 00:20:00 --qos interactive\".format(nodes=nodes)\n",
    "    cmd += \" wrap-fastframe --mpi\"\n",
    "    with open(logfilename, 'w') as logfile:\n",
    "        err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "        if err != 0:\n",
    "            print('ERROR {} running wrap-fastframe; see {}'.format(err, logfilename))\n",
    "        else:\n",
    "            print('done at {}'.format(time.asctime()))\n",
    "\n",
    "if is_fastframe_done(explist, reduxdir, verbose=True):\n",
    "    print('SUCCESS')\n",
    "\n",
    "else:\n",
    "    print('ERROR; see {}'.format(logfilename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Check individual framefile outputs\n",
    "ntot = 0\n",
    "nbad = 0\n",
    "for night, expid, flavor in explist['NIGHT', 'EXPID', 'FLAVOR']:\n",
    "    if flavor != 'flat' and flavor != 'science':\n",
    "        continue\n",
    "\n",
    "    for channel in ['b', 'r', 'z']:\n",
    "        for spectrograph in range(10):\n",
    "            camera = channel + str(spectrograph)\n",
    "            framefile = desispec.io.findfile('frame', night, expid, camera)\n",
    "            ntot += 1\n",
    "            if not os.path.exists(framefile):\n",
    "                nbad += 1\n",
    "                print('Missing {} frame {}'.format(flavor, framefile))\n",
    "\n",
    "if nbad > 0:\n",
    "    print('Missing {}/{} frame files'.format(nbad, ntot))\n",
    "else:\n",
    "    print('All {} science and flat frame files generated'.format(ntot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the spectro pipeline\n",
    "### First, create the production database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipedbfile = desispec.io.get_pipe_database()\n",
    "\n",
    "if not os.path.exists(pipedbfile):\n",
    "    cmd = \"desi_pipe create --db-sqlite --force\"\n",
    "    cmd += \" --data {}\".format(os.getenv('DESI_SPECTRO_DATA'))\n",
    "    cmd += \" --redux {}\".format(os.getenv('DESI_SPECTRO_REDUX'))\n",
    "    cmd += \" --prod {}\".format(os.getenv('SPECPROD'))\n",
    "    print(cmd)\n",
    "    err = subprocess.call(cmd.split())\n",
    "    assert err == 0\n",
    "    assert os.path.exists(desispec.io.get_pipe_database())\n",
    "    print('SUCCESS')\n",
    "    \n",
    "else:\n",
    "    print('spectro pipeline DB file already exists; skipping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync up with the actual files on disk\n",
    "\n",
    "We didn't start with raw data files, so we'll skip over extraction and PSF-fitting steps.\n",
    "`desi_pipe sync` will update the database from what files are actually on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = subprocess.call('desi_pipe sync'.split())\n",
    "assert err == 0\n",
    "output = subprocess.check_output('desi_pipe top --once'.split())\n",
    "print(output.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline scripts as a series of interactive jobs\n",
    "\n",
    "`desi_pipe chain` would be a more convenient way of doing this,\n",
    "but for the minitest it takes too long to wait for N>>1 jobs in the debug queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskdir = os.path.join(desispec.io.get_pipe_rundir(), 'minitest')\n",
    "os.makedirs(taskdir, exist_ok=True)\n",
    "\n",
    "dbpath = desispec.io.get_pipe_database()\n",
    "db = desispec.pipeline.load_db(dbpath, mode=\"w\")\n",
    "\n",
    "tasktypes = ['fiberflat', 'fiberflatnight', 'sky', 'starfit', 'fluxcalib', 'cframe', 'spectra', 'redshift']\n",
    "for tasktype in tasktypes:\n",
    "    for night in np.unique(explist['NIGHT']):\n",
    "        db.getready(night)\n",
    "\n",
    "    taskfile = \"{}/{}.tasks\".format(taskdir, tasktype)\n",
    "    cmd = \"desi_pipe tasks --tasktype {} --states ready,waiting \".format(tasktype)\n",
    "    cmd += \" > {}\".format(taskfile)\n",
    "    try:\n",
    "        subprocess.check_call(cmd, shell=True)\n",
    "    except subprocess.CalledProcessError:\n",
    "        print('FAILED: {}'.format(cmd))\n",
    "        break\n",
    "    \n",
    "    task_count = db.count_task_states(tasktype)\n",
    "    if tasktype == 'redshift':\n",
    "        ranks_per_task = 32\n",
    "        cores_per_rank = 2\n",
    "        n = task_count['ready'] * ranks_per_task // 2  #- two iterations\n",
    "        nodes = (n-1) // 32 + 1\n",
    "        runtime = 59    #- minutes\n",
    "    elif tasktype == 'spectra':\n",
    "        ranks_per_task = 1\n",
    "        cores_per_rank = 8\n",
    "        n = task_count['ready'] * ranks_per_task\n",
    "        runtime = 20    #- minutes\n",
    "    else:\n",
    "        ranks_per_task = 1\n",
    "        cores_per_rank = 2\n",
    "        n = task_count['ready'] * ranks_per_task\n",
    "        runtime = 15    #- minutes\n",
    "\n",
    "    nodes = (n*cores_per_rank-1) // 64 + 1\n",
    "\n",
    "    if n > 0:\n",
    "        t0 = time.time()\n",
    "        cmd = 'srun -A desi -t {}:00 -C haswell --qos interactive'.format(runtime)\n",
    "        cmd += ' -N {nodes} -n {procs} -c {cores} '.format(nodes=nodes, procs=n, cores=cores_per_rank)\n",
    "        cmd += ' desi_pipe_exec_mpi --tasktype {} --taskfile {}'.format(tasktype, taskfile)\n",
    "        logfilename = '{}/{}.log'.format(taskdir, tasktype)\n",
    "        print('Running {} {} tasks'.format(n, tasktype))\n",
    "        print('Logging to {}'.format(logfilename))\n",
    "        print(cmd)\n",
    "        with open(logfilename, 'w') as logfile:\n",
    "            err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "            if err != 0:\n",
    "                print('    ERROR {} for tasktype {}'.format(err, tasktype))\n",
    "                print('    See {}'.format(logfilename))\n",
    "            else:\n",
    "                dt = time.time() - t0\n",
    "                print('  DONE at {}'.format(time.asctime()))\n",
    "                print('  {} took {:.1f} min'.format(tasktype, dt/60))\n",
    "    elif task_count['waiting'] == 0 and task_count['done'] > 0:\n",
    "        print('All {} tasks already run'.format(tasktype))\n",
    "    else:\n",
    "        print('No {} tasks ready to run; skipping'.format(tasktype))\n",
    "\n",
    "for night in np.unique(explist['NIGHT']):\n",
    "    db.getready(night)\n",
    "\n",
    "print(subprocess.check_output('desi_pipe top --once'.split()).decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study job runtimes\n",
    "\n",
    "Some pipeline steps are taking longer than expected.\n",
    "It appears to be due to throttling while updating the sqlite database;\n",
    "the results are much better when using the postgres backend.  This plot\n",
    "shows the endtime vs. starttime of each fiberflat task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp(line):\n",
    "    line = line.strip()\n",
    "    h,m,s = map(int, line[-13:-13+8].split(':'))\n",
    "    return h*3600 + m*60 + s\n",
    "\n",
    "fflogs = glob.glob('{}/logs/night/*/fiberflat_*.log'.format(desispec.io.get_pipe_rundir()))\n",
    "assert(len(fflogs) > 0)\n",
    "xstart = list()\n",
    "xend = list()\n",
    "for filename in fflogs:\n",
    "    start = end = 0\n",
    "    with open(filename) as fx:\n",
    "        for line in fx:\n",
    "            if line.count(' starting at ') > 0:\n",
    "                start = parse_timestamp(line)\n",
    "            elif line.count(' done at ') > 0:\n",
    "                end = parse_timestamp(line)\n",
    "                break\n",
    "    xstart.append(start)\n",
    "    xend.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstart = np.asarray(xstart)\n",
    "xend = np.asarray(xend)\n",
    "t0 = np.min(xstart)\n",
    "xstart -= t0\n",
    "xend -= t0\n",
    "\n",
    "subplot(211)\n",
    "for a, b in zip(xstart, xend):\n",
    "    plot([a,b], [a,a])\n",
    "\n",
    "xlabel('time')\n",
    "ylabel('start time')\n",
    "\n",
    "subplot(212)\n",
    "tx = np.arange(0, np.max(xend))\n",
    "ntasks = list()\n",
    "for t in tx:\n",
    "    n = np.count_nonzero((xstart <= t) & (t+1 < xend))\n",
    "    ntasks.append(n)\n",
    "\n",
    "plot(tx, ntasks)\n",
    "ylabel('Number of running tasks')\n",
    "xlabel('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job monitoring\n",
    "\n",
    "The below isn't used by the current notebook that directly spawns srun, but it may become useful again\n",
    "if using `desi_pipe chain`, so leaving in here for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Monitor queue until those jobs are done\n",
    "def count_jobs(jobname):\n",
    "    cmd = 'squeue -u {}'.format(os.getenv('USER'))\n",
    "    proc = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = proc.communicate()\n",
    "    njob = 0\n",
    "    for line in stdout.split(b'\\n'):\n",
    "        if line.find(jobname) > 0:\n",
    "            njob += 1\n",
    "\n",
    "    return njob\n",
    "\n",
    "print('To check details, run this on cori.nersc.gov:')\n",
    "print('    squeue -u {}'.format(os.getenv('USER')))\n",
    "n = count_jobs(b'debug fiberfla')\n",
    "while n > 0:\n",
    "    nsleep = n*2\n",
    "    print('{} jobs still queued or running; sleeping {} minutes'.format(n, nsleep))\n",
    "    time.sleep(nsleep*60)\n",
    "    n = count_jobs(b'debug fiberfla')\n",
    "print('All jobs finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that we got expected outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cframe_done(explist, reduxdir, verbose=False):\n",
    "    nscience = np.count_nonzero(explist['FLAVOR'] == 'science')\n",
    "    ncframe = 30*nscience\n",
    "    cframefiles = glob.glob(reduxdir+'/exposures/*/*/cframe*.fits')\n",
    "    if verbose:\n",
    "        print('{}/{} cframe files'.format(len(cframefiles), ncframe))\n",
    "    \n",
    "    if len(cframefiles) != ncframe:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "is_cframe_done(explist, reduxdir, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_cframe_done(explist, reduxdir, verbose=True):\n",
    "    print('ERROR: missing cframe files')\n",
    "else:\n",
    "    print('All cframe files successfully generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_regrouping_done(pixels, reduxdir):\n",
    "    specfiles = glob.glob(reduxdir+'/spectra-*/*/*/spectra-*.fits')\n",
    "    #- some pixels might not be covered by real data, but most should be\n",
    "    if len(specfiles) < len(pixels) - 5:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "is_regrouping_done(pixels, reduxdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did all pixels make it from sims -> output spectra?\n",
    "\n",
    "It looks like tiles2pix was conservative and included some edge pixels that weren't\n",
    "really necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nside=64\n",
    "misspix = list()\n",
    "figure(figsize=(8,6))\n",
    "for p in pixels:\n",
    "    specfile = desispec.io.findfile('spectra', nside=nside, groupname=str(p))\n",
    "    if not os.path.exists(specfile):\n",
    "        print('Missing {}'.format(os.path.basename(specfile)))\n",
    "        misspix.append(p)\n",
    "    else:\n",
    "        fibermap = Table.read(specfile, 'FIBERMAP')\n",
    "        plot(fibermap['RA_TARGET'], fibermap['DEC_TARGET'], ',', alpha=0.5)\n",
    "\n",
    "plot_healpix(nside, misspix)\n",
    "for t in tiles:\n",
    "    plot_tile(t['RA'], t['DEC'])\n",
    "\n",
    "tmp = xlim(145, 156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_redrock_done(reduxdir, verbose=False):\n",
    "    specfiles = glob.glob(reduxdir+'/spectra-*/*/*/spectra*.fits')\n",
    "    zbestfiles = glob.glob(reduxdir+'/spectra-*/*/*/zbest*.fits')\n",
    "    if verbose:\n",
    "        print('{}/{} zbest files'.format(len(zbestfiles), len(specfiles)))\n",
    "    if len(zbestfiles) != len(specfiles):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "is_redrock_done(reduxdir, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of spectra and targets per healpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specdir = os.path.join(os.environ['DESI_SPECTRO_REDUX'], os.environ['SPECPROD'], 'spectra-64')\n",
    "specfiles = list()\n",
    "for specfile in sorted(list(desitarget.io.iter_files(specdir, 'spectra'))):\n",
    "    fm = desispec.io.read_fibermap(specfile)\n",
    "    ntarg = len(np.unique(fm['TARGETID']))\n",
    "    nspec = len(fm) * 3\n",
    "    specfiles.append( (ntarg, nspec, os.path.basename(specfile)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==================== ===== ========')\n",
    "print('specfile             nspec ntargets')\n",
    "print('==================== ===== ========')\n",
    "for ntarg, nspec, specfile in sorted(specfiles):\n",
    "    print(\"{0:20s} {1:5d} {2:8d}\".format(os.path.basename(specfile), nspec, ntarg))\n",
    "print('==================== ===== ========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create zcatalog\n",
    "\n",
    "This is just a merging of the individual zbest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcatfile = desispec.io.findfile('zcatalog')\n",
    "cmd = 'desi_zcatalog -i {reduxdir}/spectra-64 -o {zcatfile} --match {targetdir}/targets.fits'.format(\n",
    "        reduxdir=reduxdir, zcatfile=zcatfile, targetdir=targetdir)\n",
    "print(cmd)\n",
    "!$cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift QA\n",
    "\n",
    "This section does some \"by-hand\" redshift QA, bypassing the standard spectro pipeline QA (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desisim.spec_qa import redshifts as dsq_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = Table.read(os.path.join(targetdir, 'truth.fits'))\n",
    "zcat = Table.read(zcatfile)\n",
    "\n",
    "truth['TRUESPECTYPE'] = np.char.strip(truth['TRUESPECTYPE'])\n",
    "truth['TEMPLATETYPE'] = np.char.strip(truth['TEMPLATETYPE'])\n",
    "zcat['SPECTYPE'] = np.char.strip(zcat['SPECTYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.table\n",
    "del zcat.meta['EXTNAME']\n",
    "del truth.meta['EXTNAME']\n",
    "ztruth = astropy.table.join(zcat, truth, keys='TARGETID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isELG = (ztruth['TEMPLATETYPE'] == 'ELG')\n",
    "isQSO = (ztruth['TEMPLATETYPE'] == 'QSO')\n",
    "isLRG = (ztruth['TEMPLATETYPE'] == 'LRG')\n",
    "isSTAR = (ztruth['TEMPLATETYPE'] == 'STAR')\n",
    "isBGS = (ztruth['TEMPLATETYPE'] == 'BGS')\n",
    "print('QSO ', np.count_nonzero(isQSO))\n",
    "print('LRG ', np.count_nonzero(isLRG))\n",
    "print('ELG ', np.count_nonzero(isELG))\n",
    "print('STAR', np.count_nonzero(isSTAR))\n",
    "print('BGS ', np.count_nonzero(isBGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotz(ztruth, label, zmin=0.0, zmax=1.5, ylabel_=None):\n",
    "    plot(ztruth['TRUEZ'], ztruth['Z'], '.', label=label, alpha=0.1)\n",
    "    title(label)\n",
    "    xlim(zmin, zmax); ylim(zmin, zmax)\n",
    "    xlabel('True Redshift')\n",
    "    ylabel('Redrock Redshift')\n",
    "\n",
    "figure(figsize=(16,4))\n",
    "subplot(141); plotz(ztruth[isLRG], 'LRG', zmax=4)\n",
    "subplot(142); plotz(ztruth[isELG], 'ELG', zmax=4)\n",
    "subplot(143); plotz(ztruth[isQSO], 'QSO', zmax=4)\n",
    "subplot(144); plotz(ztruth[isSTAR], 'STAR', zmin=-0.002, zmax=0.002)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the truth and zcat tables\n",
    "# match_truth_z fails on the repeated TARGETID=-1 of unassigned fibers, so filter those out\n",
    "assigned = zcat['TARGETID'] >= 0\n",
    "dsq_z.match_truth_z(truth, zcat[assigned], mini_read=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('          ntarg   good  fail  miss  lost')\n",
    "for objtype in set(truth['TEMPLATETYPE']):\n",
    "    #isx = (truth['TEMPLATETYPE'] == objtype)\n",
    "    pgood, pfail, pmiss, plost, nx = dsq_z.zstats(truth, objtype=objtype)\n",
    "    #nx = np.count_nonzero(isx)\n",
    "    print('{:6s} {:8d}  {:5.1f} {:5.1f} {:5.1f} {:5.1f}'.format(objtype, nx, pgood, pfail, pmiss, plost))\n",
    "\n",
    "print()\n",
    "print('good = correct redshift and ZWARN==0')\n",
    "print('fail = bad redshift and ZWARN==0 (i.e. catastrophic failures)')\n",
    "print('miss = correct redshift ZWARN!=0 (missed opportunities)')\n",
    "print('lost = wrong redshift ZWARN!=0 (wrong but at least we know it)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Classification Confusion Matrix\n",
    "\n",
    "To be replaced with `desisim.spec_qa.redshifts.spectype_confusion` in a post-18.3 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Confusion matrix\n",
    "specmix = dict()\n",
    "spectypes = sorted(set(ztruth['TRUESPECTYPE']))\n",
    "\n",
    "for s1 in spectypes:\n",
    "    specmix[s1] = dict()\n",
    "    for s2 in spectypes:\n",
    "        n = np.count_nonzero((ztruth['TRUESPECTYPE']==s1) & (ztruth['SPECTYPE'] == s2))\n",
    "        specmix[s1][s2] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Print header line\n",
    "print('            Redrock')\n",
    "print('Truth     ', end='')\n",
    "for s1 in spectypes:\n",
    "    print('{:>8s}'.format(s1), end='')\n",
    "print()\n",
    "    \n",
    "for s1 in spectypes:\n",
    "    print('{:8s}  '.format(s1), end='')\n",
    "    for s2 in spectypes:\n",
    "        print('{:8d}'.format(specmix[s1][s2]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study target coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desitarget.targetmask import desi_mask\n",
    "from collections import Counter\n",
    "\n",
    "figure(figsize=(10,4))\n",
    "for i, targtype in enumerate(['ELG', 'LRG', 'QSO', 'STD_FSTAR', 'MWS_ANY', 'BGS_ANY']):\n",
    "    ii = (zcat['DESI_TARGET'] & desi_mask[targtype]) != 0\n",
    "    subplot(6,2,1+2*i)\n",
    "    n = hist(zcat['NUMTILE'][ii], 10, (0,10))[0]\n",
    "    text(6, np.max(n)/2, targtype)\n",
    "    xlim(0,10)\n",
    "    if i<5:\n",
    "        xticks([0,2,4,6,8,10], ['']*6)\n",
    "    subplot(6,2,2+2*i)\n",
    "    n = hist(zcat['NUMEXP'][ii], 10, (0,10))[0]\n",
    "    text(6, np.max(n)/2, targtype)\n",
    "    xlim(0,10)\n",
    "    if i<5:\n",
    "        xticks([0,2,4,6,8,10], ['']*6)\n",
    "\n",
    "subplot(6,2,11); xlabel('NUMTILE')\n",
    "subplot(6,2,12); xlabel('NUMEXP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Spectro Catalog Database\n",
    "\n",
    "Start by configuring the database, then load exposures, truth, targets, fiberassign, and the redshift catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from desispec.database.datachallenge import (setup_db, dbSession, load_file,\n",
    "                                             ObsList, Target, Truth, ZCat, FiberAssign,\n",
    "                                             load_fiberassign, load_zcat)\n",
    "options = Namespace(clobber=True, dbfile=os.path.join(basedir, 'minitest.db'), hostname=None, maxrows=0,\n",
    "                    chunksize=50000, schema=None, username=None, verbose=False, datapath=basedir)\n",
    "# We'll be using a SQLite database, ignore the return value of setup_db\n",
    "postgresql = setup_db(options)\n",
    "load_file(expfile, ObsList, hdu='EXPOSURES', expand={'PASS': 'passnum'})\n",
    "load_file(os.path.join(targetdir, 'truth.fits'), Truth, hdu='TRUTH')\n",
    "load_file(os.path.join(targetdir, 'targets.fits'), Target, hdu=\"TARGETS\")\n",
    "load_fiberassign(fibassigndir)\n",
    "load_zcat(reduxdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate the SQLAlchemy objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dbSession.query(Truth, ZCat).filter(Truth.targetid == ZCat.targetid).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q[0][0].truez, q[0][1].z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize QA output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dir to define and make\n",
    "qaprod_dir = desispec.io.qaprod_root()\n",
    "os.makedirs(qaprod_dir, exist_ok=True)\n",
    "qaprod_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Helper function for timing QA commands\n",
    "def time_command(cmd, logfile):\n",
    "    t0 = time.time()\n",
    "    print('{} RUNNING {}'.format(time.asctime(), cmd))\n",
    "    err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "    assert err == 0\n",
    "    dt = time.time() - t0\n",
    "    if dt < 60:\n",
    "        print('\"{}\" took {:.1f} seconds'.format(cmd, time.time()-t0))\n",
    "    else:\n",
    "        print('\"{}\" took {:.1f} minutes'.format(cmd, dt/60))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with the Truth\n",
    "\n",
    "The following QA uses the input truth table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_logname = os.path.join(qaprod_dir, 'qa_truth.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa0_time = time.time()\n",
    "with open(qat_logname, 'w') as logfile:\n",
    "\n",
    "    if len(glob.glob(qaprod_dir+'/QA_s2n_*')) == 10:\n",
    "        print(\"S/N figures already exist\")\n",
    "    else:\n",
    "        # S/N (~7min)\n",
    "        cmd = \"desi_qa_s2n --qaprod_dir={:s}\".format(qaprod_dir)\n",
    "        time_command(cmd, logfile)\n",
    "    \n",
    "    # zfind (~2min)\n",
    "    if (len(glob.glob(qaprod_dir+'/QA_zfind_*')) == 6) and os.path.exists(qaprod_dir+'/QA_dzsumm.png'):\n",
    "        print(\"zfind figures already exist\")\n",
    "    else:\n",
    "        cmd = \"desi_qa_zfind --yaml_file={:s}/dzsumm_stats.yaml --qaprod_dir={:s}\".format(qaprod_dir, qaprod_dir) \n",
    "        time_command(cmd, logfile)\n",
    "    \n",
    "# Time me\n",
    "print(\"Done with QA with truth at {}\".format(time.asctime()))\n",
    "qa_truth_time = time.time() - qa0_time\n",
    "print(\"That took {:.1f} minutes\".format(qa_truth_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(glob.glob(qaprod_dir+'/QA_s2n_*')) == 10\n",
    "assert len(glob.glob(qaprod_dir+'/QA_zfind_*')) == 6\n",
    "assert os.path.exists(qaprod_dir+'/QA_dzsumm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "Image(filename=qaprod_dir+'/QA_dzsumm.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=qaprod_dir+'/QA_zfind_ELG.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=qaprod_dir+'/QA_zfind_LRG.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(filename=qaprod_dir+'/QA_zfind_QSO_T.png', width=500),\n",
    "    Image(filename=qaprod_dir+'/QA_zfind_QSO_L.png', width=500),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(filename=qaprod_dir+'/QA_zfind_MWS.png', width=500),\n",
    "    Image(filename=qaprod_dir+'/QA_zfind_BGS.png', width=500),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qant_logname = os.path.join(qaprod_dir, 'qa_notruth.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the QA (if it doesn't exist already)\n",
    "qa_start_time = time.time()\n",
    "with open(qant_logname, 'w') as logfile:\n",
    "    \n",
    "    if os.path.exists(qaprod_dir+'/'+os.environ['SPECPROD']+'_qa.json'):\n",
    "        print(\"Skipping generating full prod QA file\")\n",
    "    else:\n",
    "        # Generate yaml files and figures (~30min)\n",
    "        cmd = \"desi_qa_prod --make_frameqa=3 --clobber --qaprod_dir={:s}\".format(qaprod_dir)  \n",
    "        print('{} RUNNING {}'.format(time.asctime(), cmd))\n",
    "        err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "        if err:\n",
    "            raise RuntimeError('see {}'.format(qant_logname))\n",
    "\n",
    "        # Slurp (fast)\n",
    "        cmd = \"desi_qa_prod --slurp --qaprod_dir={:s}\".format(qaprod_dir)  \n",
    "        print('{} RUNNING {}'.format(time.asctime(), cmd))\n",
    "        err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "        if err:\n",
    "            raise RuntimeError('see {}'.format(qant_logname))\n",
    "    \n",
    "    # Skyresid (~8min for the two)\n",
    "    if len(glob.glob(qaprod_dir+'/skyresid_prod_dual*')) == 3:\n",
    "        print(\"Skipping generating SkyResid figures\")\n",
    "    else:\n",
    "        cmd = \"desi_qa_skyresid --prod --qaprod_dir={:s}\".format(qaprod_dir)  \n",
    "        print('{} RUNNING {}'.format(time.asctime(), cmd))\n",
    "        err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "        if err:\n",
    "            raise RuntimeError('see {}'.format(qant_logname))\n",
    "\n",
    "    if len(glob.glob(qaprod_dir+'/skyresid_prod_gauss*')) == 3:\n",
    "        print(\"Skipping generating SkyResid Gaussianity figures\")\n",
    "    else:    \n",
    "        cmd = \"desi_qa_skyresid --gauss --qaprod_dir={:s}\".format(qaprod_dir)  \n",
    "        print('{} RUNNING {}'.format(time.asctime(), cmd))\n",
    "        err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "        if err:\n",
    "            raise RuntimeError('see {}'.format(qant_logname))\n",
    "    \n",
    "    # HTML\n",
    "    cmd = \"desi_qa_prod --html --qaprod_dir={:s}\".format(qaprod_dir)\n",
    "    print('{} RUNNING {}'.format(time.asctime(), cmd))\n",
    "    err = subprocess.call(cmd.split(), stdout=logfile, stderr=logfile)\n",
    "    if err:\n",
    "        raise RuntimeError('see {}'.format(qant_logname))\n",
    "\n",
    "    \n",
    "# Time me\n",
    "print(\"Done with QA without truth at {}\".format(time.asctime()))\n",
    "qa1_time = time.time() - qa_start_time\n",
    "print(\"That took {:.1f} minutes\".format(qa1_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file\n",
    "prod_qa = desispec.io.qa.load_qa_multiexp(qaprod_dir+'/'+os.environ['SPECPROD']+'_qa')\n",
    "assert isinstance(prod_qa, dict)\n",
    "assert len(prod_qa.keys()) == 4\n",
    "# PNGs\n",
    "assert len(glob.glob(qaprod_dir+'/skyresid_prod_dual*')) == 3\n",
    "assert len(glob.glob(qaprod_dir+'/skyresid_prod_gauss*')) == 3\n",
    "# HTML\n",
    "assert os.path.exists(qaprod_dir+'/qa-toplevel.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=qaprod_dir+'/skyresid_prod_dual_r.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done at {}\".format(time.asctime()))\n",
    "run_time = time.time() - notebook_start_time\n",
    "print(\"That took {:.1f} minutes\".format(run_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "A wishlist of things to add to this notebook\n",
    "* Target selection QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DESI 19.2",
   "language": "python",
   "name": "desi-19.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
